{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rExft8-aRL_F"
   },
   "source": [
    "<center>\n",
    "<h1><b>A Review on LSTM FCN for Time Series Classification</b></h1>\n",
    "<h2>Authors: Alex Wei, Junyu Hu, Xudong Chen\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<span><h2>__README__</h2></span><br>\n",
    "1. Recommended to run on GCP (preferably NVIDIA A100 at minimum if you want to train all models, otherwise any device should suffice for pure evaluation). If you intend to run codes in Google Colab, please first click Runtime tab and change the runtime type to GPU; then make appropriate changes to mount your drive and access other python scripts. Anaconda (with tf <= 2.10) / PyCharm / CUDA 11.x is recommended if choosing local env.\n",
    "\n",
    "2. For the dataset, please download it from our [Google Drive](https://drive.google.com/drive/u/1/folders/1YlGx9RX7Q5g9SMh2_VTB02YKwCNXuKqZ) (for this project, we used the newest 2018 version) to the same directory as this notebook. You may also refer to [UCR Time Series Classification Archive](https://www.cs.ucr.edu/~eamonn/time_series_data_2018/). However note that it is encrypted (password == `someone`) and slightly different from our version (see 3 why).\n",
    "\n",
    "3. Please be advised that we extract relevant dataset info from DataSummary.csv, which should be automatically cloned into the current directory. However, it has been modified based on the one that you directly download from UCR Archive, since as they mentioned, some datasets have missing values and varying time series length (see details in `Missing_value...` folder once you decompress). In order to give reproducible results, we here manually updated them. SO DEAR TA PLEASE USE OUR VERSION OF THIS CSV AND DATASET.\n",
    "\n",
    "# Change Log\n",
    "- 11/28 @xc2763: Created data extraction script\n",
    "- 11/29 @yw4467: Integrated data extraction into main.py; initiated the project organization on GitHub/Colab/Google Doc\n",
    "- 11/30 @yw4467: Full optimization on the project structure; created const.py to automate the info extraction\n",
    "- 11/30 @yw4467: Simplified dataset info by using a dictionary in const.py; added comments to improve readability\n",
    "- 12/01 @yw4467: Updated DataSummary.csv and UCR Archive dataset to ensure reproducibility\n",
    "- 12/02 @yw4467: Migrated visualization codes from author's repo; improved robustness of data extraction\n",
    "- 12/03 @yw4467: Implemented customized Attention LSTM (`./utils/attention.py`)\n",
    "- 12/03 @xc2763: Core bug fix for attention mechanism\n",
    "- 12/05 @yw4467: Fully optimized Attention LSTM\n",
    "- 12/05 @yw4467: Compatibility & readability improvement for TA\n",
    "- 12/12 @xc2763: Implemented data loading function in (`./utils/generics.py`)\n",
    "- 12/13 @xc2763: Loss function bug fix for model evaluation\n",
    "\n",
    "__Planned Milestones:__\n",
    "- 12/10 @yw4467: Complete first trained models\n",
    "- 12/12 @xc2763: Implement generic functions\n",
    "- 12/12 @jh4930: Implement utility functions\n",
    "- 12/14 @xc2763: Implement plot dataset function in (`./utils/generics.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend, Model, layers as l\n",
    "\n",
    "# snippet modified from Assignment 3\n",
    "dev = tf.config.list_physical_devices('GPU')\n",
    "if len(dev) > 0:\n",
    "    gpu = tf.config.experimental.get_device_details(dev[0])\n",
    "    print('Active GPU(0):', gpu['device_name'])\n",
    "    tf.config.experimental.set_memory_growth(dev[0], enable=True)\n",
    "else:\n",
    "    print('Running only on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data Extraction\n",
    "<span style=\"color: cyan\">@ Xudong Chen & Alex Wei</span> <br>\n",
    "Run this after you put the compressed dataset into current directory. If you would like to replicate results as the original paper by using the 2015 version, modify `PATH` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import zipfile\n",
    "\n",
    "PATH = 'UCRArchive_2018.zip'    # expanded, compared to 2015 version used in the original paper\n",
    "csv = 'data'                    # directory of parsed CSVs; modify as you see fit\n",
    "\n",
    "# unzip if not yet (the zip should have been auto downloaded to current dir)\n",
    "if not os.path.exists(os.path.splitext(PATH)[0]):\n",
    "    with zipfile.ZipFile(PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "\n",
    "if not os.path.exists(csv): # create output directory if doesn't exist\n",
    "    os.makedirs(csv)\n",
    "\n",
    "def extract_raw(file):\n",
    "    \"\"\"Convert each raw data file (.tsv) into .csv\"\"\"\n",
    "    out = os.path.join(csv, os.path.splitext(os.path.basename(file))[0] + '.csv')   # strip .tsv ext\n",
    "    df = pd.read_table(file, header=None, encoding='latin-1')   # load values in the dataset\n",
    "    df.fillna(0.0, inplace=True)    # fill empty time steps\n",
    "    df.to_csv(out, index=False, header=None, encoding='latin-1')\n",
    "\n",
    "files = glob.glob(os.path.join(os.path.splitext(PATH)[0], '**', '*.tsv'), recursive=True)   # find all tsv\n",
    "if files:\n",
    "    # check if all corresponding CSVs already exist\n",
    "    existing = set(glob.glob(os.path.join(csv, '*.csv')))\n",
    "    expected = {os.path.join(csv, os.path.splitext(os.path.basename(f))[0] + '.csv') for f in files}\n",
    "\n",
    "    if existing == expected:\n",
    "        print(f'All {len(files)} tsv have already been parsed. Skipping conversion.')\n",
    "    else:\n",
    "        print(f'{len(files)} tsv found. Processing...')\n",
    "        # TODO : progress bar\n",
    "        with Parallel(n_jobs=-1) as engine: engine([delayed(extract_raw)(file) for file in files])\n",
    "        print(f'All processed / saved to {csv}')\n",
    "else:\n",
    "    print('Error locating tsv files in the specified directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main - Training and Evaluation\n",
    "<span style=\"color: cyan\">@ Alex Wei</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.const import META\n",
    "from utils.keras_utils import train, eval\n",
    "from utils.attention import ALSTM\n",
    "from tensorflow.keras.initializers import Orthogonal\n",
    "\n",
    "init = Orthogonal(seed=42)  # expecting slight better performance than he_uniform\n",
    "\n",
    "# 3 layers as described in the paper\n",
    "def gen(len_ts, n_class, n_cell=8, use_att=False):\n",
    "    \"\"\" Generate the model for training with given method\n",
    "    Args:\n",
    "        len_ts: time series length;\n",
    "        n_cell: number of cells in the LSTM layer;\n",
    "        n_class: number of classes in the dataset;\n",
    "        use_att: enable attention mechanism?\n",
    "    \"\"\"\n",
    "    m = l.Input(shape=(1, len_ts))\n",
    "\n",
    "    # tf 2.x auto uses cuDNN LSTM if available\n",
    "    x = ALSTM(n_cell)(m) if use_att else l.LSTM(n_cell, recurrent_activation='sigmoid')(m)\n",
    "    x = l.Dropout(0.8)(x)\n",
    "\n",
    "    y = l.Permute((2, 1))(m) # dimension shuffle\n",
    "    y = l.Conv1D(128, 8, padding='same', kernel_initializer=init)(y)\n",
    "    y = l.BatchNormalization()(y)\n",
    "    y = l.Activation('relu')(y)\n",
    "    y = l.Conv1D(256, 5, padding='same', kernel_initializer=init)(y)\n",
    "    y = l.BatchNormalization()(y)\n",
    "    y = l.Activation('relu')(y)\n",
    "    y = l.Conv1D(128, 3, padding='same', kernel_initializer=init)(y)\n",
    "    y = l.BatchNormalization()(y)\n",
    "    y = l.Activation('relu')(y)\n",
    "    y = l.GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = l.concatenate([x, y])\n",
    "    out = l.Dense(n_class, activation='softmax')(x)\n",
    "\n",
    "    return Model(m, out)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    models = [('alstmfcn', True), ('lstmfcn', False)]\n",
    "    cells = [8, 64, 128]  # number of cells\n",
    "\n",
    "    for name, att in models:\n",
    "        for cell in cells:\n",
    "            success = []\n",
    "\n",
    "            log = f'{name}_{cell}cell_summary.csv'  # log all training results\n",
    "            if not os.path.exists(log):\n",
    "                with open(log, 'w') as file:\n",
    "                    file.write('ID, Dataset, Weight Path, Test Accuracy\\n')\n",
    "\n",
    "            for dataset in META:\n",
    "                backend.clear_session()  # release VRAM\n",
    "                file = open(log, 'a+')\n",
    "                entry, ID = dataset['Name'], dataset['ID'] - 1\n",
    "                dir_weight = f'{name}_{cell}cell/{entry}'\n",
    "                os.makedirs(f'weights/{os.path.dirname(dir_weight)}', exist_ok=True)\n",
    "\n",
    "                model = gen(dataset['Length'], dataset['Class'], cell, use_att=att)\n",
    "                print(f'{\">\" * 16} Training Dataset: {entry} {ID + 1}/{len(META)} {\"<\" * 16}')\n",
    "                # NOTE FOR TA: comment out line below for mere evaluation\n",
    "                train(model, ID, dir_weight, epochs=2400, batch_size=128, norm_ts=True)\n",
    "                acc = eval(model, ID, dir_weight, batch_size=128, norm_ts=True)\n",
    "                result = f'{ID + 1}, {entry}, {dir_weight}, {acc:.6f}\\n'\n",
    "                file.write(result)\n",
    "                file.flush()\n",
    "                success.append(result)\n",
    "                file.close()\n",
    "\n",
    "            print(f'\\n{\">\" * 16} TRAINING COMPLETE {\"<\" * 16}')\n",
    "            for line in success:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "<span style=\"color: cyan\">@ all</span>\n",
    "This section directly uses part of the source code from the author [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.keras_utils import visualize_filters, visualize_cam, visualize_context_vector\n",
    "\n",
    "# TODO: implement our own visualization functions, e.g. comparing our acc with original\n",
    "# COMMON PARAMETERS\n",
    "DATASET_ID = 0\n",
    "num_cells = 8\n",
    "\n",
    "# NEW 43 DATASET PARAMETERS\n",
    "model_name = 'alstmfcn'\n",
    "\n",
    "# visualization params\n",
    "CLASS_ID = 0\n",
    "CONV_ID = 0\n",
    "FILTER_ID = 0\n",
    "LIMIT = 1\n",
    "VISUALIZE_SEQUENCE = True\n",
    "VISUALIZE_CLASSWISE = False\n",
    "\n",
    "# script setup\n",
    "sequence_length = META[DATASET_ID]['Length']\n",
    "nb_classes = META[DATASET_ID]['Class']\n",
    "model = gen(sequence_length, nb_classes, num_cells, use_att=False)\n",
    "\n",
    "entry = META[DATASET_ID]['Name']\n",
    "name_ = f'{model_name}_{num_cells}cell/{entry}'\n",
    "\n",
    "visualize_cam(model, DATASET_ID, name_, class_id=CLASS_ID, seed=0,\n",
    "              normalize_timeseries=True)\n",
    "visualize_context_vector(model, DATASET_ID, name_, limit=LIMIT, visualize_sequence=VISUALIZE_SEQUENCE,\n",
    "                             visualize_classwise=VISUALIZE_CLASSWISE, normalize_timeseries=True)\n",
    "visualize_filters(model, DATASET_ID, name_, conv_id=CONV_ID, filter_id=FILTER_ID, seed=0,\n",
    "                      normalize_timeseries=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Karim, Fazle, et al. \"[LSTM fully convolutional networks for time series classification.](https://github.com/titu1994/LSTM-FCN)\" IEEE Access 7 (2019): 10127-10137. <br>\n",
    "[2] Wang, Zongwei, et al. \"[Time series classification from scratch with deep neural networks: A strong baseline.](https://arxiv.org/abs/1611.06455)\" 2016."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
